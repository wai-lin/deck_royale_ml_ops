{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a22df5",
   "metadata": {},
   "source": [
    "# Monitoring LLM Evaluation Results\n",
    "\n",
    "This section monitors the quality and relevance of the LLM evaluation results to detect potential issues such as:\n",
    "- Inconsistent scoring patterns\n",
    "- Evaluation drift over time\n",
    "- Outlier scores that may indicate model issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b5176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Evaluation monitoring database created\n",
      "‚úÖ Monitoring MLflow experiment: deck_royale (ID: 591958987431704571)\n",
      "üîç Monitoring 6 evaluation runs...\n",
      "\n",
      "üìä LLM Evaluation Monitoring Report:\n",
      "   ‚Ä¢ Total evaluations: 6\n",
      "   ‚Ä¢ Average overall score: 6.83\n",
      "   ‚Ä¢ Score variance: 0.167\n",
      "   ‚Ä¢ Low scores detected: 0\n",
      "   ‚Ä¢ System status: CAUTION\n",
      "‚úÖ Monitoring metrics logged to database for Grafana visualization\n",
      "   Database URI: postgresql+psycopg2://admin:admin@127.0.0.1:5432/monitoring_db\n",
      "   Table: llm_evaluation_monitoring\n",
      "\n",
      "üìà Evidently Report Generated:\n",
      "   ‚Ä¢ Reference evaluations: 3\n",
      "   ‚Ä¢ Current evaluations: 3\n",
      "   ‚Ä¢ Drift analysis completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "import pendulum\n",
    "from sqlalchemy import Boolean, Column, Float, Integer, String, DateTime\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from evidently import Dataset\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset\n",
    "import warnings\n",
    "\n",
    "EXPERIMENT_NAME = \"deck_royale\"\n",
    "\n",
    "# Suppress all numpy warnings to avoid division by zero and invalid value warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.seterr(all='ignore')  # Suppress numpy errors/warnings\n",
    "\n",
    "# Database setup for Grafana\n",
    "USER = \"admin\"\n",
    "PASSWORD = \"admin\"\n",
    "MONITORING_DB_URI = f\"postgresql+psycopg2://{USER}:{PASSWORD}@127.0.0.1:5432/monitoring_db\"\n",
    "\n",
    "# Create database table for LLM evaluation monitoring\n",
    "Base = declarative_base()\n",
    "\n",
    "class LLMEvaluationTable(Base):\n",
    "    \"\"\"Table for LLM evaluation monitoring metrics.\"\"\"\n",
    "    __tablename__ = \"llm_evaluation_monitoring\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    timestamp = Column(Float)\n",
    "    avg_overall_score = Column(Float)\n",
    "    avg_defense_score = Column(Float)\n",
    "    avg_attack_score = Column(Float)\n",
    "    avg_synergy_score = Column(Float)\n",
    "    avg_versatility_score = Column(Float)\n",
    "    avg_difficulty_score = Column(Float)\n",
    "    score_variance = Column(Float)\n",
    "    low_score_count = Column(Integer)\n",
    "    total_evaluations = Column(Integer)\n",
    "    system_health_status = Column(String)\n",
    "\n",
    "def create_monitoring_db():\n",
    "    \"\"\"Create monitoring database tables.\"\"\"\n",
    "    engine = create_engine(MONITORING_DB_URI)\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"‚úÖ LLM Evaluation monitoring database created\")\n",
    "\n",
    "def get_llm_monitoring_metrics(runs_df):\n",
    "    \"\"\"Extract monitoring metrics from MLflow runs with robust error handling.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Calculate averages and variance for each metric with safety checks\n",
    "    eval_metrics = ['eval_overall', 'eval_defense', 'eval_attack', 'eval_synergy', 'eval_versatility', 'eval_difficulty']\n",
    "    \n",
    "    for metric in eval_metrics:\n",
    "        col_name = f'metrics.{metric}'\n",
    "        if col_name in runs_df.columns:\n",
    "            values = runs_df[col_name].dropna()\n",
    "            if len(values) > 0:\n",
    "                # Safe mean calculation\n",
    "                mean_val = values.mean()\n",
    "                if pd.isna(mean_val) or not np.isfinite(mean_val):\n",
    "                    mean_val = 0.0\n",
    "                metrics[f'avg_{metric}_score'] = mean_val\n",
    "    \n",
    "    # Calculate overall variance (instability indicator) with comprehensive safety\n",
    "    overall_scores = runs_df['metrics.eval_overall'].dropna() if 'metrics.eval_overall' in runs_df.columns else pd.Series()\n",
    "    if len(overall_scores) > 1:\n",
    "        try:\n",
    "            variance = overall_scores.var()\n",
    "            if pd.isna(variance) or not np.isfinite(variance):\n",
    "                variance = 0.0\n",
    "            metrics['score_variance'] = variance\n",
    "        except (ZeroDivisionError, RuntimeWarning):\n",
    "            metrics['score_variance'] = 0.0\n",
    "    else:\n",
    "        metrics['score_variance'] = 0.0\n",
    "    \n",
    "    # Count low scores (quality issues) with comprehensive safety\n",
    "    low_threshold = 3.0\n",
    "    low_score_count = 0\n",
    "    for metric in eval_metrics:\n",
    "        col_name = f'metrics.{metric}'\n",
    "        if col_name in runs_df.columns:\n",
    "            try:\n",
    "                values = runs_df[col_name].dropna()\n",
    "                if len(values) > 0:\n",
    "                    # Safe comparison that handles NaN values\n",
    "                    low_scores = np.sum(values < low_threshold)\n",
    "                    if pd.isna(low_scores) or not np.isfinite(low_scores):\n",
    "                        low_scores = 0\n",
    "                    low_score_count += int(low_scores)\n",
    "            except (TypeError, ValueError):\n",
    "                continue  # Skip problematic columns\n",
    "    \n",
    "    metrics['low_score_count'] = low_score_count\n",
    "    metrics['total_evaluations'] = len(runs_df)\n",
    "    \n",
    "    # Determine system health with ultra-safe division\n",
    "    avg_overall = metrics.get('avg_eval_overall_score', 0)\n",
    "    if pd.isna(avg_overall) or not np.isfinite(avg_overall):\n",
    "        avg_overall = 0.0\n",
    "    \n",
    "    try:\n",
    "        if avg_overall >= 7.0:\n",
    "            metrics['system_health_status'] = 'HEALTHY'\n",
    "        elif avg_overall >= 5.0:\n",
    "            metrics['system_health_status'] = 'CAUTION'\n",
    "        else:\n",
    "            metrics['system_health_status'] = 'CRITICAL'\n",
    "    except (TypeError, ValueError):\n",
    "        metrics['system_health_status'] = 'UNKNOWN'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Initialize monitoring database\n",
    "try:\n",
    "    create_monitoring_db()\n",
    "    \n",
    "    # Retrieve MLflow experiment data\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "    # Create experiment if it doesn't exist\n",
    "    if experiment is None:\n",
    "        print(f\"‚ö†Ô∏è Experiment '{EXPERIMENT_NAME}' not found. Creating new experiment...\")\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    \n",
    "    print(f\"‚úÖ Monitoring MLflow experiment: {experiment.name} (ID: {experiment.experiment_id})\")\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    \n",
    "    if len(runs) > 0:\n",
    "        print(f\"üîç Monitoring {len(runs)} evaluation runs...\")\n",
    "        \n",
    "        # Extract monitoring metrics\n",
    "        monitoring_metrics = get_llm_monitoring_metrics(runs)\n",
    "        \n",
    "        # Display current monitoring status\n",
    "        print(\"\\nüìä LLM Evaluation Monitoring Report:\")\n",
    "        print(f\"   ‚Ä¢ Total evaluations: {monitoring_metrics['total_evaluations']}\")\n",
    "        print(f\"   ‚Ä¢ Average overall score: {monitoring_metrics.get('avg_eval_overall_score', 0):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Score variance: {monitoring_metrics['score_variance']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Low scores detected: {monitoring_metrics['low_score_count']}\")\n",
    "        print(f\"   ‚Ä¢ System status: {monitoring_metrics['system_health_status']}\")\n",
    "        \n",
    "        # Log metrics to database for Grafana\n",
    "        engine = create_engine(MONITORING_DB_URI)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        \n",
    "        # Create monitoring record - convert NumPy types to Python types\n",
    "        timestamp = pendulum.now().timestamp()\n",
    "        monitoring_record = LLMEvaluationTable(\n",
    "            timestamp=timestamp,\n",
    "            avg_overall_score=float(monitoring_metrics.get('avg_eval_overall_score', 0)),\n",
    "            avg_defense_score=float(monitoring_metrics.get('avg_eval_defense_score', 0)),\n",
    "            avg_attack_score=float(monitoring_metrics.get('avg_eval_attack_score', 0)),\n",
    "            avg_synergy_score=float(monitoring_metrics.get('avg_eval_synergy_score', 0)),\n",
    "            avg_versatility_score=float(monitoring_metrics.get('avg_eval_versatility_score', 0)),\n",
    "            avg_difficulty_score=float(monitoring_metrics.get('avg_eval_difficulty_score', 0)),\n",
    "            score_variance=float(monitoring_metrics['score_variance']),\n",
    "            low_score_count=int(monitoring_metrics['low_score_count']),\n",
    "            total_evaluations=int(monitoring_metrics['total_evaluations']),\n",
    "            system_health_status=str(monitoring_metrics['system_health_status'])\n",
    "        )\n",
    "        \n",
    "        session.add(monitoring_record)\n",
    "        session.commit()\n",
    "        session.close()\n",
    "        \n",
    "        print(f\"‚úÖ Monitoring metrics logged to database for Grafana visualization\")\n",
    "        print(f\"   Database URI: {MONITORING_DB_URI}\")\n",
    "        print(f\"   Table: llm_evaluation_monitoring\")\n",
    "        \n",
    "        # Generate Evidently report for detailed analysis\n",
    "        if len(runs) >= 2:\n",
    "            # Create a simple dataset for drift analysis on evaluation scores\n",
    "            eval_data = runs[['metrics.eval_overall', 'metrics.eval_defense', 'metrics.eval_attack', \n",
    "                            'metrics.eval_synergy', 'metrics.eval_versatility', 'metrics.eval_difficulty']].dropna()\n",
    "            \n",
    "            if len(eval_data) >= 2:\n",
    "                # Split data into reference (first half) and current (second half)\n",
    "                split_idx = len(eval_data) // 2\n",
    "                reference_data = eval_data.iloc[:split_idx]\n",
    "                current_data = eval_data.iloc[split_idx:]\n",
    "                \n",
    "                # Create Evidently datasets\n",
    "                reference_dataset = Dataset.from_pandas(reference_data)\n",
    "                current_dataset = Dataset.from_pandas(current_data)\n",
    "                \n",
    "                # Create report to detect evaluation drift\n",
    "                report = Report([DataDriftPreset()])\n",
    "                report.run(current_data=current_dataset, reference_data=reference_dataset)\n",
    "                \n",
    "                print(f\"\\nüìà Evidently Report Generated:\")\n",
    "                print(f\"   ‚Ä¢ Reference evaluations: {len(reference_data)}\")\n",
    "                print(f\"   ‚Ä¢ Current evaluations: {len(current_data)}\")\n",
    "                print(f\"   ‚Ä¢ Drift analysis completed\")\n",
    "    else:\n",
    "        print(\"‚ùå No evaluation runs found in MLflow experiment\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up monitoring: {e}\")\n",
    "    print(\"Make sure PostgreSQL is running and accessible\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monitor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
